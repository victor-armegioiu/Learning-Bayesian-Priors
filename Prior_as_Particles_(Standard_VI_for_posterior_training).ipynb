{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Prior as Particles (Standard VI for posterior training).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM32cRknIpRxtERKDQqEuR8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victor-armegioiu/Learning-Bayesian-Priors/blob/main/Prior_as_Particles_(Standard_VI_for_posterior_training).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ttj1NMzeLb7l"
      },
      "source": [
        "# Better priors represented as particles\n",
        "\n",
        "---\n",
        "\n",
        "Seed papers: FUNCTIONAL VARIATIONAL BAYESIAN NEURAL NETWORKS (https://arxiv.org/pdf/1903.05779.pdf)\n",
        "\n",
        "Understanding Variational Inference in Function-Space (https://arxiv.org/pdf/2011.09421.pdf)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HNFtn3JLmBv"
      },
      "source": [
        "## Setup \n",
        "\n",
        "Necessary imports, setting up the Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emnY7qQ_D27i"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDhq3YG2Lh57"
      },
      "source": [
        "# gpflow doesn't come preloaded with colab.\n",
        "!pip3 install gpflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ks43pRGGFeLZ",
        "outputId": "7f004b79-c6c6-45b6-e51c-f743450a483d"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from gdrive.MyDrive.prior_learning.spectral_stein_grad.estimator import SpectralScoreEstimator\n",
        "from gdrive.MyDrive.prior_learning import data_utils\n",
        "from gdrive.MyDrive.prior_learning import prior_utils\n",
        "from gdrive.MyDrive.prior_learning import sliced_score_estimation\n",
        "\n",
        "tf.compat.v1.enable_eager_execution()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpyirMceU8rl",
        "outputId": "d09f0335-d699-41b1-9e12-25ad924ce5a8"
      },
      "source": [
        "tf.executing_eagerly()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exSRRJaSZHEA",
        "outputId": "6e4147d5-103d-4047-9fd1-3f4c1554605e"
      },
      "source": [
        "tf.config.list_physical_devices('GPU') "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtFYyye2Lq1G"
      },
      "source": [
        "## Building the dataset\n",
        "\n",
        "We are going to draw `task_count` tasks, where the index sets for each tasks are drawn as $x \\sim  \\mathcal{U}(-5, 5)$, and the regression targets are generated as  $f(x) = β \\cdot x + a \\cdot \\sin(1.5 \\cdot (x − b)) + c$, where $\\beta, a, b, c$ are per-task hyperparameters.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atKQhTcWGAYp"
      },
      "source": [
        "# Draws the hyperparameters needed for each task.\n",
        "task_count = 10\n",
        "tasks_data = data_utils.GetSinusoidParams(task_count)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SC6KrF3DNlZG"
      },
      "source": [
        "config = {\n",
        "          'input_shape': 1,             # Shape of input data.\n",
        "          'size': 5,                    # Number of samples per task.  \n",
        "          'generation_method': 'sine',  # Generating process of the labels.\n",
        "          'tasks_data': tasks_data,     # Hyperparams for the sinusoids.\n",
        "}"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WI1ZtuUVxg5d"
      },
      "source": [
        "Use last `test_tasks_cnt` for testing, and leave the rest for training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rX5LSt5Jgf_"
      },
      "source": [
        "X, y = data_utils.GetDataset(config)\n",
        "X = X[:, None]\n",
        "test_tasks_cnt = 1\n",
        "\n",
        "first_test_idx = len(X) - test_tasks_cnt * config['size']\n",
        "X_train, y_train = X[:first_test_idx], y[:first_test_idx]\n",
        "X_test, y_test = X[first_test_idx:], y[first_test_idx:]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAhcM4vPgZBE",
        "outputId": "a0e78c3b-25dc-451e-ca40-a27e08fdda0d"
      },
      "source": [
        "X_train.shape, X_test.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((45, 1), (5, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WoIeHxbpzcX"
      },
      "source": [
        "scaler = StandardScaler().fit(X_train)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQZtL6nxglLY"
      },
      "source": [
        "# Use the same scaler in order to ensure that we don't inject knowledge\n",
        "# about the test distribution in our training biases.\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "lIUpN7Vkwj8A",
        "outputId": "8f2d8aae-ee11-4a71-8b59-3f1768e7e420"
      },
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "fig.suptitle('Inputs Train/Test distributions')\n",
        "ax1.hist(X_train, bins=10)\n",
        "ax2.hist(X_test, bins=10)\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ldHY_xK2Y9O"
      },
      "source": [
        "# Scale both dataset by `y_train_max`. Same reasoning\n",
        "# as in using the same scaler on inputs. This is done to avoid\n",
        "# exploding gradients.\n",
        "y_train_max = y_train.max()\n",
        "y_train /= y_train_max\n",
        "y_test /= y_train_max"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "VEdTaSi9w5iC",
        "outputId": "fc3380e7-18e7-4add-a7b6-59ab4feb02dc"
      },
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "fig.suptitle('Labels Train/Test distributions')\n",
        "ax1.hist(y_train, bins=100)\n",
        "ax2.hist(y_test, bins=100)\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUczRSqDrgyZ"
      },
      "source": [
        "## GP Prior Setup\n",
        "\n",
        "Train a $\\mathcal{GP}$ on the training data, so that we may later use the tuned kernel for estimating the covariance of a Normal distribution over function values (this shall be used as the prior in the `f-bnn` formulation below)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8reHpDqdZNH"
      },
      "source": [
        "# Pre-train GP prior.\n",
        "gp_prior, _ = prior_utils.TrainGPPrior(X_train, y_train)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp0sKmQh6O-i"
      },
      "source": [
        "## Sliced Score Estimator Setup (https://arxiv.org/pdf/1905.07088.pdf)\n",
        "\n",
        "Here we train a network $h(\\cdot, \\hat{\\theta})$ which minimizes the objective $\\mathbb{E}_{x \\sim \\mu} \\Vert h(x, \\hat{\\theta}) - \\nabla_x \\log p(x) \\Vert_{2}$ in an unsupervised way, where $\\mu$ is some unknown data generating process and $p := \\sum_i \\delta_{x_i}$ is an empirical measure represented as a set of particles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0s1vAhNu75Ax"
      },
      "source": [
        "input_shape = (1,)\n",
        "score_net = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=input_shape, name='input'),\n",
        "    tf.keras.layers.Dense(10, activation='relu'),\n",
        "    tf.keras.layers.Dense(input_shape[0]),\n",
        "])\n",
        "\n",
        "config_slice_train = {'data': y_train[:, None],\n",
        "          'score_net': score_net,\n",
        "          'epochs': 100, \n",
        "          'lambda_reg': 0.15\n",
        "}"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTktaqjF7TGu",
        "outputId": "fea6e06b-1236-4001-c28f-6c1149e24d0a"
      },
      "source": [
        "score_net, best_loss = (\n",
        "    sliced_score_estimation.GetSlicedScoreEstimator(config_slice_train,\n",
        "                                                    verbose=True))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "Epoch [0], loss: [3.894559]\n",
            "Epoch [10], loss: [3.276264]\n",
            "Epoch [20], loss: [0.376394]\n",
            "Epoch [30], loss: [-0.546520]\n",
            "Epoch [40], loss: [-1.567371]\n",
            "Epoch [50], loss: [-3.748332]\n",
            "Epoch [60], loss: [-3.213116]\n",
            "Epoch [70], loss: [-3.008458]\n",
            "Epoch [80], loss: [-6.224043]\n",
            "Epoch [90], loss: [-6.151384]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgDB6_SSLxOW"
      },
      "source": [
        "## Model setup\n",
        "\n",
        "This section covers the actual training of our particle based models, minimizing the `f-bnn` loss (likelihood term + $KL$ term), where we represent the prior as a set of particles. The prior particles themselves will be the actual **training targets**, since we regard them to be representative of the true generating process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbS_6q1rNlwg"
      },
      "source": [
        "def ApproximateEntropyGrads(estimator, samples):\n",
        "  dlog_q = estimator.compute_gradients(samples)\n",
        "  surrogate = tf.reduce_mean(\n",
        "      tf.reduce_sum(\n",
        "          tf.stop_gradient(-dlog_q) * tf.cast(samples, tf.float64), -1))\n",
        "  return surrogate"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2P4aL4kLMMg"
      },
      "source": [
        "# Estimator to be used for computing score gradients of implicit distributions\n",
        "# represtend as sets of particles.\n",
        "estimator = SpectralScoreEstimator(n_eigen_threshold=0.99, eta=0.0)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXbEKuDpL-L0"
      },
      "source": [
        "# TODO: Allow for customizable models.\n",
        "def _GetModel():\n",
        "  kl_divergence_function = (lambda q, p, _:\n",
        "                            tfp.distributions.kl_divergence(q, p) /\n",
        "                            tf.cast(len(X_train), dtype=tf.float32))\n",
        "\n",
        "  # Setup a feed forward BNN with 1 hidden layer.\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.Input(shape=X_train[0].shape,name='input'),\n",
        "      tfp.layers.DenseFlipout(10, kernel_divergence_fn=kl_divergence_function,\n",
        "                              activation=tf.nn.relu, name=\"dense_tfp_1\"),\n",
        "      tfp.layers.DenseFlipout(1,\n",
        "                              kernel_divergence_fn=kl_divergence_function,\n",
        "                              name=\"out_tfp_pred\"),\n",
        "  ])\n",
        "\n",
        "  return model"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f37ZS9Fa1_N6"
      },
      "source": [
        "Reframing variational inference in the function space, we arrive at the following objective formulation [Sun et al., 2019]\n",
        "\n",
        "\\begin{equation}\n",
        " \\log p(\\mathcal{D} \\vert f) - \\lambda KL(q \\Vert p) \\hspace{0.5cm}(1)\n",
        "\\end{equation}\n",
        "\n",
        "where $q, p$ represent the posterior, and the prior over function values.\n",
        "\n",
        "Using the reparametrization trick, function values $f(x) \\sim q(\\cdot)$ are drawn from a parameterized neural net $g_\\phi(x ; \\xi)$. Here $\\phi$ denotes the set of optimizable parameters, while $\\xi$ is a random perturbation under which the change of variable is performed. Hence, drawing samples $f(x) \\sim q$ is done by forwarding $x$ through $g_\\phi(\\cdot ; \\xi)$.\n",
        "\n",
        "In order to obtain a feasible optimization procedure from $(1)$, the $KL$ divergence gradient is then expanded as \n",
        "\n",
        "\\begin{equation}\n",
        " \\nabla_\\phi KL(q \\Vert p)  = \\mathbb{E}_{\\xi}[\\nabla_\\phi \\textbf{f} (\\nabla_\\textbf{f} \\log q(\\textbf{f}) - \\nabla_\\textbf{f}\\log p(\\textbf{f})  )] \\hspace{0.5cm}(2)\n",
        "\\end{equation}\n",
        "\n",
        "Note that the $KL$ term is made tractable by using SSGE [Shi et al., 2018] to extract gradients for the score functions, since both $q(\\cdot), p(\\cdot)$ are both implicit distributions represented as particles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17AdJLvodP8K"
      },
      "source": [
        "# Draw samples from the variational posterior.\n",
        "def SamplePosterior(variational_posterior, n_particles, x, training=False):\n",
        "  return tf.stack([variational_posterior(x, training=training)  \n",
        "                    for _ in range(n_particles)])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imt5TrXCIrC_"
      },
      "source": [
        "def Train(model=_GetModel(),\n",
        "          epochs=200,\n",
        "          n_particles=10,\n",
        "          task_size=5,\n",
        "          prior_particles=np.copy(y_train),\n",
        "          use_functional_kl=True,\n",
        "          lambda_kl=1.0,\n",
        "          method='sliced_score_estimation',\n",
        "          verbose=True):\n",
        "\n",
        "  criterion = tf.keras.losses.MeanSquaredError()\n",
        "  optimizer = tf.keras.optimizers.Adam(0.001)\n",
        "  test_losses = [] # Record losses we see during testing.\n",
        "\n",
        "  # The real targets are the true prior.\n",
        "  if len(prior_particles.shape) == 2:\n",
        "    prior_particles = np.squeeze(prior_particles, -1)\n",
        "\n",
        "  # Configs used as arguments for the `ComputeCrossEntropy` function, which\n",
        "  # will be used for computing the log_prior gradients\n",
        "  # (or approximate gradients). The method field indicates which log-prior\n",
        "  # gradient scheme we will use. Only 'gp' comes with an analytical form.\n",
        "  # for the gradients, trading off expresiveness.\n",
        "  configs = {\n",
        "    'gp': {'method': 'gp',  'kernel_function': gp_prior.kernel},\n",
        "\n",
        "    'ssge': {'method': 'ssge', 'estimator': estimator,\n",
        "            'n_particles': n_particles, 'prior_particles': prior_particles},\n",
        "\n",
        "    'sliced_score_estimation': {'method': 'sliced', \n",
        "                                'score_estimator': score_net},\n",
        "  }\n",
        "\n",
        "  if use_functional_kl:\n",
        "    print('Using %s for computing log-prior gradients. \\n' % method)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    task_idx = np.random.choice(np.arange(task_count - test_tasks_cnt))\n",
        "    start, end = task_idx * task_size, (task_idx + 1) * task_size \n",
        "    \n",
        "    with tf.GradientTape() as g:\n",
        "      predictions = SamplePosterior(model, n_particles, X_train[start:end],\n",
        "                                    True)\n",
        "      log_likelihood = -criterion(tf.reduce_mean(predictions, axis=0),\n",
        "                                  y_train[start:end])\n",
        "      log_likelihood_copy = log_likelihood.numpy()\n",
        "      \n",
        "      if use_functional_kl:\n",
        "          # Use SSGE to approximate the log gradients of the functional\n",
        "          # posterior given the 'drawn' samples from the neural net.\n",
        "          # See Equation (2).\n",
        "          posterior_samples = (\n",
        "              predictions + np.random.normal(0, 1, size=predictions.shape))\n",
        "          entropy_sur = ApproximateEntropyGrads(estimator, posterior_samples)\n",
        "        \n",
        "          if 'gp' in method:\n",
        "            configs[method]['x'] = X_train[start:end]\n",
        "            configs[method]['y'] = tf.squeeze(posterior_samples, axis=2)\n",
        "          elif 'ssge' in method:\n",
        "            configs[method]['y'] = posterior_samples\n",
        "          elif 'score' in method:\n",
        "            configs[method]['y'] = tf.reshape(posterior_samples,\n",
        "                [n_particles *  len(y_train[start:end]), -1])\n",
        "            \n",
        "          cross_entropy_sur = prior_utils.ComputeCrossEntropy(configs[method])\n",
        "          \n",
        "          # Add these up to produce the KL term to be optimized.\n",
        "          functional_kl = (tf.cast(entropy_sur, tf.float32) -\n",
        "                            tf.cast(cross_entropy_sur, tf.float32))\n",
        "          log_likelihood += (lambda_kl * tf.cast(functional_kl, tf.float64) /\n",
        "                            len(X_train[start:end]))\n",
        "    \n",
        "      grads = g.gradient(-log_likelihood, model.trainable_weights)\n",
        "      optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "    if verbose and epoch % 50 == 0:\n",
        "      test_predictions = SamplePosterior(model, n_particles, X_test, False)\n",
        "      test_log_likelihood = -criterion(tf.reduce_mean(predictions, axis=0), \n",
        "                                      y_test)\n",
        "      print('Epoch [%d] test log-likelihood: [%f]' %\n",
        "            (epoch, test_log_likelihood,))\n",
        "      test_losses.append(test_log_likelihood.numpy())\n",
        "      \n",
        "  return (model, test_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7_Tlu0v4BRY"
      },
      "source": [
        "## Results\n",
        "\n",
        "We run the models in two regimes:\n",
        "\n",
        "\n",
        "\n",
        "1.   Just use $MLE$ for training, no functional $KL$.\n",
        "2.   Use the full `f-bnn` objective (Eq. (1)), where we perform the log-prior gradient estimation with each of : `Sliced Score Estimation`, `GP Prior`, `Spectral Stein Gradient Estimator`.\n",
        "\n",
        "We run the training loop `tries_per_method` times for each of the 4 above combinations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybP0bwGSReVq",
        "outputId": "e16da47f-0c4e-48ae-a096-d4bda0e61482"
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "statistics = defaultdict(list)\n",
        "tries_per_method = 3\n",
        "\n",
        "for method in [(False, 'mle'), (True, 'sliced_score_estimation'),\n",
        "               (True, 'ssge'), (True, 'gp')]:\n",
        "  for i in range(tries_per_method):\n",
        "    print(method, i)\n",
        "    model, test_losses = Train(use_functional_kl=method[0],\n",
        "                              method=method[1], verbose=True)\n",
        "    statistics[method].append(test_losses[-1])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(False, 'mle') 0\n",
            "Epoch [0] test log-likelihood: [-0.035488]\n",
            "Epoch [50] test log-likelihood: [-0.043097]\n",
            "Epoch [100] test log-likelihood: [-0.034992]\n",
            "Epoch [150] test log-likelihood: [-0.028489]\n",
            "(False, 'mle') 1\n",
            "Epoch [0] test log-likelihood: [-0.039183]\n",
            "Epoch [50] test log-likelihood: [-0.033278]\n",
            "Epoch [100] test log-likelihood: [-0.040513]\n",
            "Epoch [150] test log-likelihood: [-0.025199]\n",
            "(False, 'mle') 2\n",
            "Epoch [0] test log-likelihood: [-0.037234]\n",
            "Epoch [50] test log-likelihood: [-0.029327]\n",
            "Epoch [100] test log-likelihood: [-0.042094]\n",
            "Epoch [150] test log-likelihood: [-0.042524]\n",
            "(True, 'sliced_score_estimation') 0\n",
            "Using sliced_score_estimation for computing log-prior gradients. \n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/MyDrive/prior_learning/spectral_stein_grad/estimator/spectral.py:72: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "Epoch [0] test log-likelihood: [-0.026206]\n",
            "Epoch [50] test log-likelihood: [-0.022022]\n",
            "Epoch [100] test log-likelihood: [-0.031119]\n",
            "Epoch [150] test log-likelihood: [-0.021398]\n",
            "(True, 'sliced_score_estimation') 1\n",
            "Using sliced_score_estimation for computing log-prior gradients. \n",
            "\n",
            "Epoch [0] test log-likelihood: [-0.031394]\n",
            "Epoch [50] test log-likelihood: [-0.028908]\n",
            "Epoch [100] test log-likelihood: [-0.025943]\n",
            "Epoch [150] test log-likelihood: [-0.018865]\n",
            "(True, 'sliced_score_estimation') 2\n",
            "Using sliced_score_estimation for computing log-prior gradients. \n",
            "\n",
            "Epoch [0] test log-likelihood: [-0.028675]\n",
            "Epoch [50] test log-likelihood: [-0.026922]\n",
            "Epoch [100] test log-likelihood: [-0.024269]\n",
            "Epoch [150] test log-likelihood: [-0.023374]\n",
            "(True, 'ssge') 0\n",
            "Using ssge for computing log-prior gradients. \n",
            "\n",
            "Epoch [0] test log-likelihood: [-0.019375]\n",
            "Epoch [50] test log-likelihood: [-0.025907]\n",
            "Epoch [100] test log-likelihood: [-0.034833]\n",
            "Epoch [150] test log-likelihood: [-0.035289]\n",
            "(True, 'ssge') 1\n",
            "Using ssge for computing log-prior gradients. \n",
            "\n",
            "Epoch [0] test log-likelihood: [-0.021356]\n",
            "Epoch [50] test log-likelihood: [-0.021801]\n",
            "Epoch [100] test log-likelihood: [-0.022710]\n",
            "Epoch [150] test log-likelihood: [-0.030521]\n",
            "(True, 'ssge') 2\n",
            "Using ssge for computing log-prior gradients. \n",
            "\n",
            "Epoch [0] test log-likelihood: [-0.027224]\n",
            "Epoch [50] test log-likelihood: [-0.027351]\n",
            "Epoch [100] test log-likelihood: [-0.026410]\n",
            "Epoch [150] test log-likelihood: [-0.025888]\n",
            "(True, 'gp') 0\n",
            "Using gp for computing log-prior gradients. \n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/distributions/distribution.py:298: MultivariateNormalFullCovariance.__init__ (from tensorflow_probability.python.distributions.mvn_full_covariance) is deprecated and will be removed after 2019-12-01.\n",
            "Instructions for updating:\n",
            "`MultivariateNormalFullCovariance` is deprecated, use `MultivariateNormalTriL(loc=loc, scale_tril=tf.linalg.cholesky(covariance_matrix))` instead.\n",
            "Epoch [0] test log-likelihood: [-0.043582]\n",
            "Epoch [50] test log-likelihood: [-0.045326]\n",
            "Epoch [100] test log-likelihood: [-0.028137]\n",
            "Epoch [150] test log-likelihood: [-0.026305]\n",
            "(True, 'gp') 1\n",
            "Using gp for computing log-prior gradients. \n",
            "\n",
            "Epoch [0] test log-likelihood: [-0.026850]\n",
            "Epoch [50] test log-likelihood: [-0.026032]\n",
            "Epoch [100] test log-likelihood: [-0.030007]\n",
            "Epoch [150] test log-likelihood: [-0.026117]\n",
            "(True, 'gp') 2\n",
            "Using gp for computing log-prior gradients. \n",
            "\n",
            "Epoch [0] test log-likelihood: [-0.025332]\n",
            "Epoch [50] test log-likelihood: [-0.024234]\n",
            "Epoch [100] test log-likelihood: [-0.024695]\n",
            "Epoch [150] test log-likelihood: [-0.023459]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rl58_ZNuXqRW",
        "outputId": "be55b9e4-11f9-4961-b684-33080dc6bc6d"
      },
      "source": [
        "# Show confidence intervals for all results corresponding to each method.\n",
        "import scipy.stats as st\n",
        "\n",
        "for method, results in statistics.items():\n",
        "  print(method[1], ': ', st.t.interval(0.95, len(results) - 1,\n",
        "                                 loc=np.mean(results), scale=st.sem(results)))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mle :  (-0.054926711804943046, -0.009214515748345104)\n",
            "sliced_score_estimation :  (-0.02682774522168263, -0.015596957174312831)\n",
            "ssge :  (-0.04224282658340274, -0.018889731106740845)\n",
            "gp :  (-0.029247469877773893, -0.021340258741801607)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}