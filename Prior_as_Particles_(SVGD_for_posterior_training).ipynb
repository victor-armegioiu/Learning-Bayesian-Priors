{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Prior as Particles (SVGD for posterior training).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPJblEpp2mkF6VgsV0qd2v9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victor-armegioiu/Learning-Bayesian-Priors/blob/main/Prior_as_Particles_(SVGD_for_posterior_training).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ttj1NMzeLb7l"
      },
      "source": [
        "# Better priors represented as particles\n",
        "\n",
        "---\n",
        "\n",
        "Seed papers: FUNCTIONAL VARIATIONAL BAYESIAN NEURAL NETWORKS (https://arxiv.org/pdf/1903.05779.pdf)\n",
        "\n",
        "Understanding Variational Inference in Function-Space (https://arxiv.org/pdf/2011.09421.pdf)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HNFtn3JLmBv"
      },
      "source": [
        "## Setup \n",
        "\n",
        "Necessary imports, setting up the Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emnY7qQ_D27i"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDhq3YG2Lh57"
      },
      "source": [
        "# gpflow doesn't come preloaded with colab.\n",
        "!pip3 install gpflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ks43pRGGFeLZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e756fda-aa3c-4ef5-97e6-1999904ec8c1"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from gdrive.MyDrive.prior_learning.spectral_stein_grad.estimator import SpectralScoreEstimator\n",
        "from gdrive.MyDrive.prior_learning import data_utils\n",
        "from gdrive.MyDrive.prior_learning import prior_utils\n",
        "from gdrive.MyDrive.prior_learning import sliced_score_estimation\n",
        "from gdrive.MyDrive.prior_learning import svgd\n",
        "\n",
        "\n",
        "tf.compat.v1.enable_eager_execution()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpyirMceU8rl",
        "outputId": "3037e78b-3817-468e-d3ea-133cdc57cccd"
      },
      "source": [
        "tf.executing_eagerly()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exSRRJaSZHEA",
        "outputId": "07744c0e-19ec-4d8a-abf7-78f1dddf9b3f"
      },
      "source": [
        "tf.config.list_physical_devices('GPU') "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtFYyye2Lq1G"
      },
      "source": [
        "## Building the dataset\n",
        "\n",
        "We are going to draw `task_count` tasks, where the index sets for each tasks are drawn as $x \\sim  \\mathcal{U}(-5, 5)$, and the regression targets are generated as  $f(x) = β \\cdot x + a \\cdot \\sin(1.5 \\cdot (x − b)) + c$, where $\\beta, a, b, c$ are per-task hyperparameters.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atKQhTcWGAYp"
      },
      "source": [
        "# Draws the hyperparameters needed for each task.\n",
        "task_count = 10\n",
        "tasks_data = data_utils.GetSinusoidParams(task_count)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SC6KrF3DNlZG"
      },
      "source": [
        "task_size = 5\n",
        "\n",
        "config = {\n",
        "        'input_shape': 1,             # Shape of input data.\n",
        "        'size': task_size,            # Number of samples per task.  \n",
        "        'generation_method': 'sine',  # Generating process of the labels.\n",
        "        'tasks_data': tasks_data,     # Hyperparams for the sinusoids.\n",
        "}"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48tEDtfSu9tu"
      },
      "source": [
        "Use last `test_tasks_cnt` for testing, and leave the rest for training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rX5LSt5Jgf_"
      },
      "source": [
        "X, y = data_utils.GetDataset(config)\n",
        "X = X[:, None]\n",
        "test_tasks_cnt = 1\n",
        "\n",
        "first_test_idx = len(X) - test_tasks_cnt * config['size']\n",
        "X_train, y_train = X[:first_test_idx], y[:first_test_idx]\n",
        "X_test, y_test = X[first_test_idx:], y[first_test_idx:]"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAhcM4vPgZBE",
        "outputId": "cbcf9595-5e19-4d18-8020-3a35c1ebcdba"
      },
      "source": [
        "X_train.shape, X_test.shape"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((45, 1), (5, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WoIeHxbpzcX"
      },
      "source": [
        "scaler = StandardScaler().fit(X_train)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQZtL6nxglLY"
      },
      "source": [
        "# Use the same scaler in order to ensure that we don't inject knowledge\n",
        "# about the test distribution in our training biases.\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "lIUpN7Vkwj8A",
        "outputId": "469a5633-7480-4e55-ede0-99313e746f39"
      },
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "fig.suptitle('Inputs Train/Test distributions')\n",
        "ax1.hist(X_train, bins=10)\n",
        "ax2.hist(X_test, bins=10)\n",
        "plt.show()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ldHY_xK2Y9O"
      },
      "source": [
        "# Scale both dataset by `y_train_max`. Same reasoning\n",
        "# as in using the same scaler on inputs. This is done to avoid\n",
        "# exploding gradients.\n",
        "y_train_max = y_train.max()\n",
        "y_train /= y_train_max\n",
        "y_test /= y_train_max"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "VEdTaSi9w5iC",
        "outputId": "802ccc0c-945a-4c82-e427-83742ef4277e"
      },
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "fig.suptitle('Labels Train/Test distributions')\n",
        "ax1.hist(y_train, bins=100)\n",
        "ax2.hist(y_test, bins=100)\n",
        "plt.show()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUczRSqDrgyZ"
      },
      "source": [
        "## GP Prior Setup\n",
        "\n",
        "Train a $\\mathcal{GP}$ on the training data, so that we may later use the tuned kernel for estimating the covariance of a Normal distribution over function values (this shall be used as the prior in the `f-bnn` formulation below)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8reHpDqdZNH"
      },
      "source": [
        "# Pre-train GP prior.\n",
        "gp_prior, _ = prior_utils.TrainGPPrior(X_train, y_train)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp0sKmQh6O-i"
      },
      "source": [
        "## Sliced Score Estimator Setup (https://arxiv.org/pdf/1905.07088.pdf)\n",
        "\n",
        "Here we train a network $h(\\cdot, \\hat{\\theta})$ which minimizes the objective $\\mathbb{E}_{x \\sim \\mu} \\Vert h(x, \\hat{\\theta}) - \\nabla_x \\log p(x) \\Vert_{2}$ in an unsupervised way, where $\\mu$ is some unknown data generating process and $p := \\sum_i \\delta_{x_i}$ is an empirical measure represented as a set of particles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0s1vAhNu75Ax"
      },
      "source": [
        "input_shape = (1,)\n",
        "score_net = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=input_shape, name='input'),\n",
        "    tf.keras.layers.Dense(10, activation='relu'),\n",
        "    tf.keras.layers.Dense(input_shape[0]),\n",
        "])\n",
        "\n",
        "config_slice_train = {'data': y_train[:, None],\n",
        "          'score_net': score_net,\n",
        "          'epochs': 100, \n",
        "          'lambda_reg': 0.15\n",
        "}"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTktaqjF7TGu",
        "outputId": "f5c57bc7-36ea-4831-bb8e-d7b522db654d"
      },
      "source": [
        "score_net, best_loss = (\n",
        "    sliced_score_estimation.GetSlicedScoreEstimator(config_slice_train,\n",
        "                                                    verbose=True))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [0], loss: [5.190279]\n",
            "Epoch [10], loss: [5.695688]\n",
            "Epoch [20], loss: [0.952549]\n",
            "Epoch [30], loss: [2.481444]\n",
            "Epoch [40], loss: [-0.916463]\n",
            "Epoch [50], loss: [-2.213982]\n",
            "Epoch [60], loss: [-0.976804]\n",
            "Epoch [70], loss: [-1.878082]\n",
            "Epoch [80], loss: [-2.169096]\n",
            "Epoch [90], loss: [-9.094554]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgDB6_SSLxOW"
      },
      "source": [
        "## Model setup\n",
        "\n",
        "This section covers the actual training of our particle based models, minimizing the `f-bnn` loss (likelihood term + $KL$ term), where we represent the prior as a set of particles. The prior particles themselves will be the actual **training targets**, since we regard them to be representative of the true generating process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2P4aL4kLMMg"
      },
      "source": [
        "# Estimator to be used for computing score gradients of implicit distributions\n",
        "# represtend as sets of particles.\n",
        "estimator = SpectralScoreEstimator(n_eigen_threshold=0.99, eta=0.0)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckeHM_HHSgV_"
      },
      "source": [
        "# An ensemble of these models will represent our set of particles.\n",
        "def GetLinearModel():\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Dense(5, input_shape=(1,), activation='relu'))\n",
        "  model.add(tf.keras.layers.Dense(1))\n",
        "  return model"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f37ZS9Fa1_N6"
      },
      "source": [
        "Reframing variational inference in the function space, we arrive at the following objective formulation [Sun et al., 2019]\n",
        "\n",
        "\\begin{equation}\n",
        " \\log p(\\mathcal{D} \\vert f) - \\lambda KL(q \\Vert p) \\hspace{0.5cm}(1)\n",
        "\\end{equation}\n",
        "\n",
        "where $q, p$ represent the posterior, and the prior over function values.\n",
        "\n",
        "Using the reparametrization trick, function values $f(x) \\sim q(\\cdot)$ are drawn from a parameterized neural net $g_\\phi(x ; \\xi)$. Here $\\phi$ denotes the set of optimizable parameters, while $\\xi$ is a random perturbation under which the change of variable is performed. Hence, drawing samples $f(x) \\sim q$ is done by forwarding $x$ through $g_\\phi(\\cdot ; \\xi)$.\n",
        "\n",
        "In order to obtain a feasible optimization procedure from $(1)$, the $KL$ divergence gradient is then expanded as \n",
        "\n",
        "\\begin{equation}\n",
        " \\nabla_\\phi KL(q \\Vert p)  = \\mathbb{E}_{\\xi}[\\nabla_\\phi \\textbf{f} (\\nabla_\\textbf{f} \\log q(\\textbf{f}) - \\nabla_\\textbf{f}\\log p(\\textbf{f})  )] \\hspace{0.5cm}(2)\n",
        "\\end{equation}\n",
        "\n",
        "Note that the $KL$ term is made tractable by using SSGE [Shi et al., 2018] to extract gradients for the score functions, since both $q(\\cdot), p(\\cdot)$ are both implicit distributions represented as particles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbS_6q1rNlwg"
      },
      "source": [
        "def ApproximateEntropyGrads(estimator, samples):\n",
        "  dlog_q = estimator.compute_gradients(samples)\n",
        "  surrogate = tf.reduce_mean(\n",
        "      tf.reduce_sum(\n",
        "          tf.stop_gradient(-dlog_q) * tf.cast(samples, tf.float64), -1))\n",
        "  return surrogate"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlZfdrdbv-N6"
      },
      "source": [
        "def SamplePosterior(particles, x, training=True):\n",
        "  return tf.stack([particle(x, training=training) for particle in particles])"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvN3HW-omGvl"
      },
      "source": [
        "def Train(epochs=200,\n",
        "          n_particles=10,\n",
        "          task_size=5,\n",
        "          prior_particles=np.copy(y_train),\n",
        "          use_functional_kl=True,\n",
        "          lambda_kl=1.0,\n",
        "          method='sliced_score_estimation',\n",
        "          verbose=True):\n",
        "  \n",
        "  particles = [GetLinearModel() for _ in range(n_particles)]\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(0.001)\n",
        "  criterion = tf.keras.losses.MeanSquaredError()\n",
        "  test_losses = [] # Record losses we see during testing.\n",
        "\n",
        "\n",
        "  # The real targets are the true prior.\n",
        "  if len(prior_particles.shape) == 2:\n",
        "    prior_particles = np.squeeze(prior_particles, -1)\n",
        "\n",
        "  # Configs used as arguments for the `ComputeCrossEntropy` function, which\n",
        "  # will be used for computing the log_prior gradients\n",
        "  # (or approximate gradients). The method field indicates which log-prior\n",
        "  # gradient scheme we will use. Only 'gp' comes with an analytical form.\n",
        "  # for the gradients, trading off expresiveness.\n",
        "  configs = {\n",
        "    'gp': {'method': 'gp',  'kernel_function': gp_prior.kernel},\n",
        "\n",
        "    'ssge': {'method': 'ssge', 'estimator': estimator,\n",
        "            'n_particles': n_particles, 'prior_particles': prior_particles},\n",
        "\n",
        "    'sliced_score_estimation': {'method': 'sliced', \n",
        "                                'score_estimator': score_net},\n",
        "  }\n",
        "\n",
        "  if use_functional_kl:\n",
        "    print('Using %s for computing log-prior gradients. \\n' % method)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    task_idx = np.random.choice(np.arange(task_count - test_tasks_cnt))\n",
        "    start, end = task_idx * task_size, (task_idx + 1) * task_size \n",
        "    \n",
        "    with tf.GradientTape(persistent=True) as g:\n",
        "      predictions = SamplePosterior(particles, X_train[start:end], \n",
        "                                    training=True)\n",
        "      log_likelihood = -criterion(tf.reduce_mean(predictions, axis=0),\n",
        "                                  y_train[start:end])\n",
        "      log_likelihood_copy = log_likelihood.numpy()\n",
        "      \n",
        "      if use_functional_kl:\n",
        "          # Use SSGE to approximate the log gradients of the functional\n",
        "          # posterior given the 'drawn' samples from the neural net.\n",
        "          # See Equation (2).\n",
        "          posterior_samples = (\n",
        "              predictions + np.random.normal(0, 1, size=predictions.shape))\n",
        "          entropy_sur = ApproximateEntropyGrads(estimator, posterior_samples)\n",
        "        \n",
        "          if 'gp' in method:\n",
        "            configs[method]['x'] = X_train[start:end]\n",
        "            configs[method]['y'] = tf.squeeze(posterior_samples, axis=2)\n",
        "          elif 'ssge' in method:\n",
        "            configs[method]['y'] = posterior_samples\n",
        "          elif 'score' in method:\n",
        "            configs[method]['y'] = tf.reshape(posterior_samples,\n",
        "                [n_particles *  len(y_train[start:end]), -1])\n",
        "            \n",
        "          cross_entropy_sur = prior_utils.ComputeCrossEntropy(configs[method])\n",
        "          \n",
        "          # Add these up to produce the KL term to be optimized.\n",
        "          functional_kl = (tf.cast(entropy_sur, tf.float32) -\n",
        "                            tf.cast(cross_entropy_sur, tf.float32))\n",
        "          log_likelihood += (lambda_kl * tf.cast(functional_kl, tf.float64) /\n",
        "                            len(X_train[start:end]))\n",
        "\n",
        "    grads_list = []\n",
        "    vars_list = []\n",
        "\n",
        "    for particle in particles:\n",
        "      grads = g.gradient(log_likelihood, particle.trainable_variables)\n",
        "      grads_list.append(grads)\n",
        "      vars_list.append(particle.trainable_variables)\n",
        "\n",
        "    # Gradient ascent for log likelihood and SVGD kernel repulsive forces.\n",
        "    svgd_optimizer = svgd.SVGDOptimizer(grads_list, vars_list, optimizer)\n",
        "    svgd_optimizer.Optimize()\n",
        "\n",
        "    if verbose and epoch % 50 == 0:\n",
        "      test_predictions = SamplePosterior(particles, X_test, training=False)\n",
        "      test_log_likelihood = -criterion(tf.reduce_mean(predictions, axis=0), \n",
        "                                      y_test)\n",
        "      print('Epoch [%d] test log-likelihood: [%f]' %\n",
        "            (epoch, test_log_likelihood,))\n",
        "      test_losses.append(test_log_likelihood.numpy())\n",
        "\n",
        "  return (particles, test_losses)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcDiOVif24Tj"
      },
      "source": [
        "## Results\n",
        "\n",
        "We run the models in two regimes:\n",
        "\n",
        "\n",
        "\n",
        "1.   Just use $MLE$ for training, no functional $KL$.\n",
        "2.   Use the full `f-bnn` objective (Eq. (1)), where we perform the log-prior gradient estimation with each of : `Sliced Score Estimation`, `GP Prior`, `Spectral Stein Gradient Estimator`.\n",
        "\n",
        "We run the training loop `tries_per_method` times for each of the 4 above combinations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNzmFb1SyhRi",
        "outputId": "08b07976-f68f-4696-d674-0db3b02b29ba"
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "statistics = defaultdict(list)\n",
        "tries_per_method = 3\n",
        "\n",
        "for method in [(False, 'mle'), (True, 'sliced_score_estimation'),\n",
        "               (True, 'ssge'), (True, 'gp')]:\n",
        "  for i in range(tries_per_method):\n",
        "    print(method, i)\n",
        "    particles, test_losses = Train(use_functional_kl=method[0],\n",
        "                              method=method[1], verbose=True)\n",
        "    statistics[method].append(test_losses[-1])"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(False, 'mle') 0\n",
            "Epoch [0] test log-likelihood: [-0.132790]\n",
            "Epoch [50] test log-likelihood: [-0.082830]\n",
            "Epoch [100] test log-likelihood: [-0.083582]\n",
            "Epoch [150] test log-likelihood: [-0.084937]\n",
            "(False, 'mle') 1\n",
            "Epoch [0] test log-likelihood: [-0.118366]\n",
            "Epoch [50] test log-likelihood: [-0.139616]\n",
            "Epoch [100] test log-likelihood: [-0.098878]\n",
            "Epoch [150] test log-likelihood: [-0.109007]\n",
            "(False, 'mle') 2\n",
            "Epoch [0] test log-likelihood: [-0.102962]\n",
            "Epoch [50] test log-likelihood: [-0.075252]\n",
            "Epoch [100] test log-likelihood: [-0.101153]\n",
            "Epoch [150] test log-likelihood: [-0.139954]\n",
            "(True, 'sliced_score_estimation') 0\n",
            "Using sliced_score_estimation for computing log-prior gradients. \n",
            "\n",
            "Epoch [0] test log-likelihood: [-0.078537]\n",
            "Epoch [50] test log-likelihood: [-0.157820]\n",
            "Epoch [100] test log-likelihood: [-0.290087]\n",
            "Epoch [150] test log-likelihood: [-0.347122]\n",
            "(True, 'sliced_score_estimation') 1\n",
            "Using sliced_score_estimation for computing log-prior gradients. \n",
            "\n",
            "Epoch [0] test log-likelihood: [-0.119972]\n",
            "Epoch [50] test log-likelihood: [-0.203507]\n",
            "Epoch [100] test log-likelihood: [-0.409857]\n",
            "Epoch [150] test log-likelihood: [-0.345299]\n",
            "(True, 'sliced_score_estimation') 2\n",
            "Using sliced_score_estimation for computing log-prior gradients. \n",
            "\n",
            "Epoch [0] test log-likelihood: [-0.147040]\n",
            "Epoch [50] test log-likelihood: [-0.201632]\n",
            "Epoch [100] test log-likelihood: [-0.284117]\n",
            "Epoch [150] test log-likelihood: [-0.439757]\n",
            "(True, 'ssge') 0\n",
            "Using ssge for computing log-prior gradients. \n",
            "\n",
            "Epoch [0] test log-likelihood: [-0.085966]\n",
            "Epoch [50] test log-likelihood: [-0.075896]\n",
            "Epoch [100] test log-likelihood: [-0.107169]\n",
            "Epoch [150] test log-likelihood: [-0.163283]\n",
            "(True, 'ssge') 1\n",
            "Using ssge for computing log-prior gradients. \n",
            "\n",
            "Epoch [0] test log-likelihood: [-0.155844]\n",
            "Epoch [50] test log-likelihood: [-0.093711]\n",
            "Epoch [100] test log-likelihood: [-0.087019]\n",
            "Epoch [150] test log-likelihood: [-0.099834]\n",
            "(True, 'ssge') 2\n",
            "Using ssge for computing log-prior gradients. \n",
            "\n",
            "Epoch [0] test log-likelihood: [-0.090281]\n",
            "Epoch [50] test log-likelihood: [-0.076394]\n",
            "Epoch [100] test log-likelihood: [-0.073016]\n",
            "Epoch [150] test log-likelihood: [-0.093302]\n",
            "(True, 'gp') 0\n",
            "Using gp for computing log-prior gradients. \n",
            "\n",
            "Epoch [0] test log-likelihood: [-0.151737]\n",
            "Epoch [50] test log-likelihood: [-0.108090]\n",
            "Epoch [100] test log-likelihood: [-0.093184]\n",
            "Epoch [150] test log-likelihood: [-0.082340]\n",
            "(True, 'gp') 1\n",
            "Using gp for computing log-prior gradients. \n",
            "\n",
            "Epoch [0] test log-likelihood: [-0.095372]\n",
            "Epoch [50] test log-likelihood: [-0.085416]\n",
            "Epoch [100] test log-likelihood: [-0.084217]\n",
            "Epoch [150] test log-likelihood: [-0.079167]\n",
            "(True, 'gp') 2\n",
            "Using gp for computing log-prior gradients. \n",
            "\n",
            "Epoch [0] test log-likelihood: [-0.090930]\n",
            "Epoch [50] test log-likelihood: [-0.091380]\n",
            "Epoch [100] test log-likelihood: [-0.087350]\n",
            "Epoch [150] test log-likelihood: [-0.087853]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoKyA0X_ymUQ",
        "outputId": "7a052787-7388-45d9-f9d2-c38d11e4b831"
      },
      "source": [
        "# Show confidence intervals for all results corresponding to each method.\n",
        "import scipy.stats as st\n",
        "\n",
        "for method, results in statistics.items():\n",
        "  print(method[1], ': ', st.t.interval(0.95, len(results) - 1,\n",
        "                                 loc=np.mean(results), scale=st.sem(results)))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mle :  (-0.17981282326730708, -0.04278592315164585)\n",
            "sliced_score_estimation :  (-0.5115779564028239, -0.24320718395260424)\n",
            "ssge :  (-0.2148337122673209, -0.02277842592453802)\n",
            "gp :  (-0.09403860336253506, -0.07220109039833683)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}