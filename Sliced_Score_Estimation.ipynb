{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sliced Score Estimation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMkZFtSqA8FB4MaDeiJ4/sd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victor-armegioiu/Learning-Bayesian-Priors/blob/main/Sliced_Score_Estimation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJtMLK8cinyq"
      },
      "source": [
        "# Sliced Score Matching (https://arxiv.org/pdf/1905.07088.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3OJreLHdNmw"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pprint"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-95Bn1M5dU7C"
      },
      "source": [
        "data = np.random.randn(10000)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "tTaqn6qrdiL1",
        "outputId": "9fc34bb2-2d19-4309-e46c-b12df6a7d06e"
      },
      "source": [
        "plt.hist(data, density=True, bins=100)\n",
        "plt.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPpDJ0JcoKgX"
      },
      "source": [
        "Implements the *sliced score estimator* objective function, and returns a trained NN, denoted as $h(\\cdot, \\hat{\\theta})$. Ideally, given an implicit distribution represented as a set of particles (samples $x \\sim p(\\cdot)$), the trained model learns to predict the score function of $p$, in an unsupervised fashion, such that $\\Vert h(x, \\hat{\\theta}) - \\nabla_x \\log p(x) \\Vert$ is minimized. \n",
        "\n",
        "Note that we do this **without** access to the actual score function.\n",
        "We use samples $x \\sim \\mathcal{N}(0, 1)$, since $\\nabla_x \\log p(x) = -x$ in this case, so it's easier to check for correct results just by looking at them.\n",
        "See **3.2 SLICED SCORE ESTIMATION** for details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWGpDHiwWAME"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "def GetSlicedScoreEstimator(config, print_every_n=10, verbose=False):\n",
        "  \"\"\"Train a NN to perform score estimation based on samples.\n",
        "\n",
        "  @params:\n",
        "    config: dict, contains training setting, the samples and the model to be  \n",
        "    optimized.\n",
        "\n",
        "    print_every_n: number of epochs to wait before printing loss info.\n",
        "    verbose: Printing is done only if this is set to `True`.\n",
        "  \"\"\"\n",
        "  x = config['data']\n",
        "  score_net = config['score_net']\n",
        "\n",
        "  # Training setup.\n",
        "  epochs = config['epochs']\n",
        "  lambda_reg = config['lambda_reg']\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "  best_model = tf.keras.models.clone_model(score_net)\n",
        "  best_loss = 1e9\n",
        "\n",
        "  x_tensor = tf.cast(x, tf.float64)\n",
        "  for epoch in range(epochs):\n",
        "    with tf.GradientTape(persistent=True) as g:\n",
        "      g.watch(x_tensor)\n",
        "      vectors = tf.random.normal(shape=x_tensor.shape, dtype=tf.float64)\n",
        "      \n",
        "      grad1 = tf.cast(score_net(x_tensor), tf.float64)\n",
        "      gradv = tf.math.reduce_sum(grad1 * vectors, axis=0)\n",
        "\n",
        "      loss1 = lambda_reg * tf.math.reduce_sum(\n",
        "          (grad1 * vectors) ** 2, axis=0) \n",
        "\n",
        "      grad2 = g.gradient(gradv, x_tensor)\n",
        "      loss2 = tf.math.reduce_sum(vectors * grad2, axis=0)\n",
        "\n",
        "      # Combine the losses.\n",
        "      loss_val = tf.reduce_mean(loss1 + loss2)\n",
        "\n",
        "    grads = g.gradient(loss_val, score_net.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, score_net.trainable_weights))\n",
        "\n",
        "    if loss_val.numpy() < best_loss:\n",
        "      best_loss = loss_val.numpy()\n",
        "      best_model.set_weights(score_net.get_weights())\n",
        "\n",
        "    if verbose and epoch % print_every_n == 0:\n",
        "      print('Epoch [%d], loss: [%f]' % (epoch, loss_val.numpy(),))\n",
        "\n",
        "  return best_model, best_loss\n",
        "    "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AntZ5Gdoe9h_",
        "outputId": "70410fb6-08b2-4ff8-a350-29c165f081bf"
      },
      "source": [
        "input_shape = (1,)\n",
        "score_net = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=input_shape, name='input'),\n",
        "    tf.keras.layers.Dense(10, activation='relu'),\n",
        "    tf.keras.layers.Dense(input_shape[0]),\n",
        "])\n",
        "\n",
        "config = {'data': data[:, None],\n",
        "          'score_net': score_net,\n",
        "          'epochs': 1000, \n",
        "          'lambda_reg': 0.15\n",
        "}\n",
        "\n",
        "best_score_estimator, best_loss = GetSlicedScoreEstimator(config, verbose=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "Epoch [0], loss: [-1707.479708]\n",
            "Epoch [10], loss: [-1913.203701]\n",
            "Epoch [20], loss: [-2093.652093]\n",
            "Epoch [30], loss: [-2365.111260]\n",
            "Epoch [40], loss: [-2515.873124]\n",
            "Epoch [50], loss: [-2749.246060]\n",
            "Epoch [60], loss: [-2690.968213]\n",
            "Epoch [70], loss: [-2970.881006]\n",
            "Epoch [80], loss: [-3159.448414]\n",
            "Epoch [90], loss: [-3180.307620]\n",
            "Epoch [100], loss: [-3420.451418]\n",
            "Epoch [110], loss: [-3599.202437]\n",
            "Epoch [120], loss: [-3722.238804]\n",
            "Epoch [130], loss: [-3778.214210]\n",
            "Epoch [140], loss: [-4031.416459]\n",
            "Epoch [150], loss: [-4091.968501]\n",
            "Epoch [160], loss: [-4389.781549]\n",
            "Epoch [170], loss: [-4493.239384]\n",
            "Epoch [180], loss: [-4503.510026]\n",
            "Epoch [190], loss: [-4740.960536]\n",
            "Epoch [200], loss: [-4597.640830]\n",
            "Epoch [210], loss: [-4991.424514]\n",
            "Epoch [220], loss: [-5207.473622]\n",
            "Epoch [230], loss: [-5437.842315]\n",
            "Epoch [240], loss: [-5443.153971]\n",
            "Epoch [250], loss: [-5629.808508]\n",
            "Epoch [260], loss: [-5754.116041]\n",
            "Epoch [270], loss: [-5773.548479]\n",
            "Epoch [280], loss: [-5971.231381]\n",
            "Epoch [290], loss: [-5991.357861]\n",
            "Epoch [300], loss: [-5908.713879]\n",
            "Epoch [310], loss: [-6350.033862]\n",
            "Epoch [320], loss: [-6194.112707]\n",
            "Epoch [330], loss: [-6093.096842]\n",
            "Epoch [340], loss: [-6307.234246]\n",
            "Epoch [350], loss: [-6430.529986]\n",
            "Epoch [360], loss: [-6535.939793]\n",
            "Epoch [370], loss: [-6481.805721]\n",
            "Epoch [380], loss: [-6354.060849]\n",
            "Epoch [390], loss: [-6266.406144]\n",
            "Epoch [400], loss: [-6505.814437]\n",
            "Epoch [410], loss: [-6532.394884]\n",
            "Epoch [420], loss: [-6524.298949]\n",
            "Epoch [430], loss: [-6755.437455]\n",
            "Epoch [440], loss: [-6741.173029]\n",
            "Epoch [450], loss: [-6826.225145]\n",
            "Epoch [460], loss: [-6671.452111]\n",
            "Epoch [470], loss: [-6535.638970]\n",
            "Epoch [480], loss: [-6461.716899]\n",
            "Epoch [490], loss: [-6801.600129]\n",
            "Epoch [500], loss: [-6837.766309]\n",
            "Epoch [510], loss: [-6424.517425]\n",
            "Epoch [520], loss: [-6653.090233]\n",
            "Epoch [530], loss: [-6523.114742]\n",
            "Epoch [540], loss: [-6387.434458]\n",
            "Epoch [550], loss: [-6578.210310]\n",
            "Epoch [560], loss: [-6463.740402]\n",
            "Epoch [570], loss: [-6524.325575]\n",
            "Epoch [580], loss: [-6215.637447]\n",
            "Epoch [590], loss: [-6411.300171]\n",
            "Epoch [600], loss: [-6098.970199]\n",
            "Epoch [610], loss: [-6010.131012]\n",
            "Epoch [620], loss: [-6315.774131]\n",
            "Epoch [630], loss: [-6492.902271]\n",
            "Epoch [640], loss: [-6106.179677]\n",
            "Epoch [650], loss: [-6150.829617]\n",
            "Epoch [660], loss: [-6379.654607]\n",
            "Epoch [670], loss: [-6086.426044]\n",
            "Epoch [680], loss: [-5942.839809]\n",
            "Epoch [690], loss: [-5892.116524]\n",
            "Epoch [700], loss: [-6196.258825]\n",
            "Epoch [710], loss: [-6127.983037]\n",
            "Epoch [720], loss: [-5638.026743]\n",
            "Epoch [730], loss: [-5878.408543]\n",
            "Epoch [740], loss: [-5773.767601]\n",
            "Epoch [750], loss: [-5710.197695]\n",
            "Epoch [760], loss: [-5797.451691]\n",
            "Epoch [770], loss: [-5934.252141]\n",
            "Epoch [780], loss: [-5719.030060]\n",
            "Epoch [790], loss: [-5771.584678]\n",
            "Epoch [800], loss: [-5750.491987]\n",
            "Epoch [810], loss: [-5665.442133]\n",
            "Epoch [820], loss: [-5676.221229]\n",
            "Epoch [830], loss: [-5582.939800]\n",
            "Epoch [840], loss: [-5784.927020]\n",
            "Epoch [850], loss: [-5654.827658]\n",
            "Epoch [860], loss: [-5669.045723]\n",
            "Epoch [870], loss: [-5765.153308]\n",
            "Epoch [880], loss: [-5607.521661]\n",
            "Epoch [890], loss: [-5650.186845]\n",
            "Epoch [900], loss: [-5932.952676]\n",
            "Epoch [910], loss: [-6026.053112]\n",
            "Epoch [920], loss: [-5805.380148]\n",
            "Epoch [930], loss: [-5631.652879]\n",
            "Epoch [940], loss: [-5577.157226]\n",
            "Epoch [950], loss: [-5687.582044]\n",
            "Epoch [960], loss: [-5749.748998]\n",
            "Epoch [970], loss: [-5690.001983]\n",
            "Epoch [980], loss: [-5887.157760]\n",
            "Epoch [990], loss: [-5683.832467]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogkOnZgFnNTW",
        "outputId": "4e54e213-d94f-47b7-c81f-e3822544adf4"
      },
      "source": [
        "print('Lowest loss:', best_loss)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lowest loss: -6962.4145226680985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbeexqeufO5h",
        "outputId": "22622cc5-b070-4934-d6c0-a3f0b95d2563"
      },
      "source": [
        "test_data = np.random.randn(5)\n",
        "scores = best_score_estimator.predict(test_data[:, None])\n",
        "scores = [score[0] for score in scores]\n",
        "\n",
        "pprint.pprint(list(zip(test_data, scores)))\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0.08620048598248987, -0.07383168),\n",
            " (-0.38355502449083445, -0.04168506),\n",
            " (3.058310617341467, -3.6396065),\n",
            " (-1.188687658138537, 0.6873603),\n",
            " (2.4908088889753377, -2.6998122)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Or4veyT1npLR"
      },
      "source": [
        "Optimally, the plotted points would fall on the same line (this only holds for a unit variance, $0$-mean, univariate Gaussian distribution), and the scales would be the same on both axes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "EzJoru1hnjH0",
        "outputId": "6fd49f65-6696-413a-be4f-5eacfa90aa0c"
      },
      "source": [
        "plt.scatter(test_data, scores)\n",
        "plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}